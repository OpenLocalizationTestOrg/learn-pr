In the context of analytics, data streams are event data generated by sensors or other sources that can be analyzed by another technology. Analyzing a data stream is typically done to measure the state change of a component or to capture information on an area of interest. The intent being to:

- Continuously analyze data to detect issues and understand or respond to them.
- Understand component or system behavior under various conditions to fuel further enhancements of said component or system.
- Trigger specific actions when certain thresholds are identified.

In today's world, data streams are ubiquitous. Companies can harness the latent knowledge in data streams to improve efficiencies and further innovation. Examples of use cases that analyze data streams include:

:::row:::
    :::column:::
- Stock market trends.
- Monitoring data of water pipelines and electrical transmission and distribution systems by utility companies.
- Mechanical component health monitoring data in automotive and automobile industries.
- Monitoring data from industrial and manufacturing equipment.
    :::column-end:::
    :::column:::
- Sensor data in transportation, such as traffic management and highway toll lanes.
- Patient health monitoring data in the healthcare industry.
- Satellite data in the space industry.
- Fraud detection in the banking and finance industries.
- Sentiment analysis of social media posts.
    :::column-end:::
:::row-end:::

## Approaches to data stream processing

There are two approaches to processing data streams: on-demand and live. 

Streaming data can be collected over time and persisted in storage as static data. The data can then be processed when convenient or during times when compute costs are lower. The downside to this approach is the cost of storing the data. 

In contrast, live data streams have relatively low storage requirements. They also require more processing power to run computations in sliding windows over continuously incoming data to generate the insights.