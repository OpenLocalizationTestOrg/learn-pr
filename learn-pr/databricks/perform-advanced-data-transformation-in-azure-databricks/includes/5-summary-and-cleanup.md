Data transformation is an important aspect of the ETL process. It can be as simple as applying schema to incoming data to restructure it. Or it can be more complex, where you perform customized transformations. The complexity of the transformation is based on the type of raw data and the requirements of specific scenarios. 

In Apache Spark and Azure Databricks, you can perform simple transformation by using built-in functions. You can also use user-defined functions to perform customized and complex transformations.

## Clean up

If you plan on completing other Azure Databricks modules, don't delete your Azure Databricks instance yet. You can use the same environment for the other modules.

### Delete the Azure Databricks instance

1. Navigate to the Azure portal.
2. Navigate to the Resource Group that contains your Azure Databricks instance.
3. Select **Delete resource group**.
4. Type the name of the resource group in the confirmation text box.
5. Select **Delete**.